#!/usr/bin/env python3
"""term_dictionary_updater.py
æ—¢å­˜ã®å°‚é–€ç”¨èªè¾æ›¸ã®é–¢é€£èªã‚’æ›´æ–°ã™ã‚‹ãƒ„ãƒ¼ãƒ«
------------------------------------------------
æ—¢å­˜ã®å°‚é–€ç”¨èªã‚’ã‚¯ã‚¨ãƒªã¨ã—ã¦PGVectorã§é¡ä¼¼æ–‡è„ˆã‚’æ¤œç´¢ã—ã€
æ–°ãŸãªé–¢é€£èªã‚’ç™ºè¦‹ã—ã¦è¾æ›¸ã‚’æ›´æ–°ã™ã‚‹
"""

from __future__ import annotations

import asyncio
import json
import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any, Set
from datetime import datetime

from dotenv import load_dotenv
from sqlalchemy import create_engine, text

# Project-specific imports
sys.path.append(str(Path(__file__).resolve().parents[1]))
from rag.config import Config

# term_extractor_embedingã‹ã‚‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.scripts.term_extractor_embeding import (
    VectorStore,
    SynonymDetector,
    embeddings
)

# â”€â”€ ENV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
cfg = Config()

PG_URL = f"postgresql+psycopg://{cfg.db_user}:{cfg.db_password}@{cfg.db_host}:{cfg.db_port}/{cfg.db_name}"
JARGON_TABLE_NAME = cfg.jargon_table_name

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# â”€â”€ Term Dictionary Updater â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TermDictionaryUpdater:
    """æ—¢å­˜ç”¨èªã®é–¢é€£èªã‚’æ›´æ–°ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.vector_store = VectorStore(connection_string, "term_extraction_chunks")
        self.synonym_detector = SynonymDetector()
        self.update_log = []
        
    def get_existing_terms(self) -> List[Dict[str, Any]]:
        """jargon_dictionaryã‹ã‚‰æ—¢å­˜ã®ç”¨èªã‚’å–å¾—"""
        engine = create_engine(self.connection_string)
        query = text(f"""
            SELECT term, definition, domain, aliases, created_at, updated_at
            FROM {JARGON_TABLE_NAME}
            ORDER BY term
        """)
        
        terms = []
        try:
            with engine.connect() as conn:
                result = conn.execute(query)
                for row in result:
                    terms.append({
                        'term': row.term,
                        'definition': row.definition,
                        'domain': row.domain,
                        'aliases': row.aliases or [],
                        'created_at': row.created_at,
                        'updated_at': row.updated_at
                    })
            logger.info(f"Retrieved {len(terms)} existing terms from database")
        except Exception as e:
            logger.error(f"Error retrieving terms from database: {e}")
            
        return terms
    
    async def search_related_contexts(self, term: str, definition: str) -> List[str]:
        """ç”¨èªã¨å®šç¾©ã‚’ã‚¯ã‚¨ãƒªã¨ã—ã¦é¡ä¼¼æ–‡è„ˆã‚’æ¤œç´¢"""
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if not self.vector_store.vector_store:
            logger.warning("Vector store not initialized. Skipping context search.")
            return []
        
        # ã‚¯ã‚¨ãƒªã‚’æ§‹ç¯‰ï¼ˆç”¨èªã¨å®šç¾©ã®æœ€åˆã®éƒ¨åˆ†ã‚’ä½¿ç”¨ï¼‰
        query = f"{term} {definition[:100]}"
        
        try:
            # é¡ä¼¼æ–‡è„ˆã‚’æ¤œç´¢
            results = await self.vector_store.vector_store.asimilarity_search_with_score(
                query=query,
                k=10  # 10ä»¶å–å¾—
            )
            
            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’æŠ½å‡º
            contexts = []
            for doc, score in results:
                if score > 0.5:  # é¡ä¼¼åº¦ãŒ0.5ä»¥ä¸Šã®ã‚‚ã®ã ã‘
                    contexts.append(doc.page_content)
            
            logger.debug(f"Found {len(contexts)} related contexts for term: {term}")
            return contexts
            
        except Exception as e:
            logger.error(f"Error searching contexts for {term}: {e}")
            return []
    
    def extract_new_synonyms(self, term: str, contexts: List[str], existing_aliases: List[str]) -> List[str]:
        """æ–‡è„ˆã‹ã‚‰æ–°ã—ã„é–¢é€£èªã‚’æŠ½å‡º"""
        if not contexts:
            return []
        
        # å…¨æ–‡è„ˆã‚’çµåˆ
        combined_text = "\n".join(contexts)
        
        # æ–‡è„ˆã‹ã‚‰å€™è£œèªã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        import re
        from collections import Counter
        
        # ç”¨èªã®å‘¨è¾ºã«å‡ºç¾ã™ã‚‹èªã‚’åé›†
        pattern = rf'(.{{0,50}}){re.escape(term)}(.{{0,50}})'
        matches = re.findall(pattern, combined_text, re.IGNORECASE)
        
        nearby_words = []
        for before, after in matches:
            # å‰å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©ã£ã½ã„ã‚‚ã®ã‚’æŠ½å‡º
            words = re.findall(r'[ã‚¡-ãƒ¶ãƒ¼]+|[a-zA-Z0-9]+', before + after)
            nearby_words.extend(words)
        
        # é »å‡ºèªã‚’å€™è£œã¨ã™ã‚‹
        word_counts = Counter(nearby_words)
        candidates = [word for word, count in word_counts.items() if count >= 2 and len(word) >= 2]
        
        # æ—¢å­˜ã®é–¢é€£èªã¨é‡è¤‡ã—ãªã„ã‚‚ã®ã‚’é¸æŠ
        existing_set = set(existing_aliases + [term])
        new_synonyms = []
        
        for candidate in candidates:
            if candidate not in existing_set:
                # ç·¨é›†è·é›¢ã§é¡ä¼¼æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                from difflib import SequenceMatcher
                similarity = SequenceMatcher(None, term.lower(), candidate.lower()).ratio()
                
                # é¡ä¼¼åº¦ãŒé«˜ã„å ´åˆï¼ˆ70-95%ï¼‰ã¯é–¢é€£èªã¨ã—ã¦æ¡ç”¨
                if 0.7 < similarity < 0.95:
                    new_synonyms.append(candidate)
                # ã¾ãŸã¯ç”¨èªã‚’å«ã‚€è¤‡åˆèª
                elif term in candidate or candidate in term:
                    if len(candidate) > 2:  # çŸ­ã™ãã‚‹èªã¯é™¤å¤–
                        new_synonyms.append(candidate)
        
        # é‡è¤‡ã‚’é™¤å»
        new_synonyms = list(set(new_synonyms))
        
        if new_synonyms:
            logger.info(f"Found {len(new_synonyms)} new synonyms for {term}: {new_synonyms}")
        
        return new_synonyms
    
    def update_term_in_db(self, term: str, new_aliases: List[str]) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç”¨èªã‚’æ›´æ–°"""
        if not new_aliases:
            return False
        
        engine = create_engine(self.connection_string)
        
        try:
            with engine.begin() as conn:
                # æ—¢å­˜ã®é–¢é€£èªã‚’å–å¾—
                get_query = text(f"""
                    SELECT aliases FROM {JARGON_TABLE_NAME}
                    WHERE term = :term
                """)
                result = conn.execute(get_query, {"term": term}).fetchone()
                
                if result:
                    existing_aliases = result.aliases or []
                    # æ–°ã—ã„é–¢é€£èªã‚’æ—¢å­˜ã®ã‚‚ã®ã¨çµ±åˆ
                    all_aliases = list(set(existing_aliases + new_aliases))
                    
                    # æ›´æ–°
                    update_query = text(f"""
                        UPDATE {JARGON_TABLE_NAME}
                        SET aliases = :aliases,
                            updated_at = CURRENT_TIMESTAMP
                        WHERE term = :term
                    """)
                    conn.execute(update_query, {
                        "term": term,
                        "aliases": all_aliases
                    })
                    
                    logger.info(f"Updated term '{term}' with {len(new_aliases)} new aliases")
                    return True
                    
        except Exception as e:
            logger.error(f"Error updating term {term}: {e}")
            
        return False
    
    async def update_all_terms(self) -> Dict[str, Any]:
        """å…¨ã¦ã®æ—¢å­˜ç”¨èªã®é–¢é€£èªã‚’æ›´æ–°"""
        logger.info("Starting term dictionary update...")
        
        # 1. æ—¢å­˜ç”¨èªã‚’å–å¾—
        existing_terms = self.get_existing_terms()
        if not existing_terms:
            logger.warning("No existing terms found in database")
            return {"updated_count": 0, "changes": []}
        
        # 2. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’åˆæœŸåŒ–ï¼ˆæ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½¿ç”¨ï¼‰
        try:
            # æ—¢å­˜ã®ãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚‹ã‹ç¢ºèª
            engine = create_engine(self.connection_string)
            with engine.connect() as conn:
                result = conn.execute(text("""
                    SELECT COUNT(*) as count 
                    FROM langchain_pg_collection 
                    WHERE name = 'term_extraction_chunks'
                """)).fetchone()
                
                if result and result.count > 0:
                    logger.info("Using existing vector store")
                    # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½¿ç”¨
                    from langchain_community.vectorstores import PGVector
                    self.vector_store.vector_store = PGVector(
                        collection_name="term_extraction_chunks",
                        connection_string=self.connection_string,
                        embedding_function=embeddings
                    )
                else:
                    logger.warning("No existing vector store found. Please run term extraction first.")
                    return {"updated_count": 0, "changes": []}
                    
        except Exception as e:
            logger.error(f"Error initializing vector store: {e}")
            return {"updated_count": 0, "changes": []}
        
        # 3. å„ç”¨èªã‚’å‡¦ç†
        updated_terms = []
        
        for term_data in existing_terms:
            term = term_data['term']
            definition = term_data['definition']
            existing_aliases = term_data['aliases']
            
            logger.info(f"Processing term: {term}")
            
            # é¡ä¼¼æ–‡è„ˆã‚’æ¤œç´¢
            contexts = await self.search_related_contexts(term, definition)
            
            if contexts:
                # æ–°ã—ã„é–¢é€£èªã‚’æŠ½å‡º
                new_synonyms = self.extract_new_synonyms(term, contexts, existing_aliases)
                
                if new_synonyms:
                    # DBã‚’æ›´æ–°
                    if self.update_term_in_db(term, new_synonyms):
                        updated_terms.append({
                            'term': term,
                            'new_synonyms': new_synonyms,
                            'old_synonyms': existing_aliases,
                            'total_contexts': len(contexts)
                        })
        
        # 4. æ›´æ–°ãƒ­ã‚°ã‚’ä½œæˆ
        result = {
            'updated_count': len(updated_terms),
            'total_terms': len(existing_terms),
            'changes': updated_terms,
            'timestamp': datetime.now().isoformat()
        }
        
        # ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_path = Path("output/term_update_log.json")
        output_path.parent.mkdir(exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Update complete. Updated {len(updated_terms)}/{len(existing_terms)} terms")
        logger.info(f"Update log saved to {output_path}")
        
        return result

# â”€â”€ Main Execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    updater = TermDictionaryUpdater(PG_URL)
    result = await updater.update_all_terms()
    
    # çµæœã‚’è¡¨ç¤º
    print("\n" + "="*50)
    print("Term Dictionary Update Results")
    print("="*50)
    print(f"Total terms processed: {result['total_terms']}")
    print(f"Terms updated: {result['updated_count']}")
    
    if result['changes']:
        print("\nUpdated terms:")
        for change in result['changes'][:10]:  # æœ€åˆã®10ä»¶ã ã‘è¡¨ç¤º
            print(f"\nğŸ“ {change['term']}")
            print(f"   Old synonyms: {change['old_synonyms']}")
            print(f"   New synonyms: {change['new_synonyms']}")
            print(f"   Contexts found: {change['total_contexts']}")
    
    print(f"\nFull log saved to: output/term_update_log.json")

if __name__ == "__main__":
    asyncio.run(main())