# RAGシステム評価UI ガイド

## 概要

RAGシステムの評価機能がStreamlit UIに統合されました。この機能により、Webブラウザを通じて直感的にRAGシステムの性能を評価できます。

## アクセス方法

1. Streamlitアプリを起動
```bash
streamlit run app.py
```

2. ブラウザでアプリにアクセス
3. **🎯 Evaluation** タブをクリック

## 評価機能の構成

### 1. 📊 基本評価

最も基本的な評価機能で、以下の機能を提供します：

**使用される検索設定（固定）**
- **クエリ拡張**: 無効
- **RAG Fusion**: 無効  
- **専門用語拡張**: 有効
- **リランキング**: 有効
- **検索タイプ**: ハイブリッド検索（デフォルト）

#### データ入力方法

**CSVアップロード**
- カスタム評価データをCSVファイルでアップロード
- 必要な形式：`質問,想定の引用元1,想定の引用元2,想定の引用元3`
- ファイル形式検証とプレビュー機能

**手動入力**
- Webインターフェースで直接テストケースを入力
- 動的にテストケースの追加・削除が可能
- リアルタイムでデータ検証

#### 評価設定

**類似度計算手法**
- `azure_embedding`: Azure OpenAI埋め込み（推奨）
- `azure_llm`: GPT-4oによる直接判定
- `text_overlap`: 単語重複度
- `hybrid`: 埋め込み+重複度の混合

**信頼性閾値**
- スライダーで0.1-0.9の範囲で調整
- デフォルト：0.7（厳格な評価）
- 動的に評価基準を変更可能

**評価K値設定**
- K=1, 3, 5, 10から選択
- 複数K値での同時評価
- Recall@K, Precision@K, nDCG@Kを計算

#### 結果出力
- CSVファイルでの結果エクスポート
- ブラウザからの直接ダウンロード
- カスタムファイル名設定

### 2. 📈 詳細分析

高度な比較分析機能：

#### 包括的評価
- 複数の類似度手法での比較評価
- 異なるデータセットでの性能比較
- 手法間の性能差異を可視化

#### 分析オプション
- **質問別詳細結果**: 各質問の個別評価結果
- **グラフ可視化**: レーダーチャートと棒グラフ
- **信頼性閾値分析**: 閾値変化による性能影響分析

#### 可視化機能
- **レーダーチャート**: 複数指標の全体比較
- **棒グラフ**: 手法別性能比較
- **インタラクティブグラフ**: Plotlyによる動的可視化

### 3. 📋 カスタム評価

柔軟な個別評価機能：

#### 単一質問テスト
- 任意の質問でRAGシステムをテスト
- リアルタイムでの回答生成と評価
- 期待される回答との類似度比較

#### 検索オプション設定
- **クエリ拡張**: 質問の語彙拡張
- **RAG Fusion**: 複数クエリによる検索強化
- **専門用語拡張**: 専門用語辞書活用
- **リランキング**: 検索結果の再順位付け
- **検索タイプ**: ハイブリッド/ベクトル/キーワード検索

#### 詳細分析
- **取得文書表示**: 検索で取得された関連文書
- **メタデータ表示**: 文書のメタ情報
- **使用トークン数**: API使用量の詳細
- **推定コスト**: 評価実行コストの算出

## 評価指標の説明

### 主要指標

**MRR (Mean Reciprocal Rank)**
- 最初の関連文書が現れる順位の逆数の平均
- 値域：0-1（高いほど良い）
- 検索精度の基本的指標

**Recall@K**
- 上位K件中に含まれる関連文書の割合
- 値域：0-1（高いほど良い）
- 網羅性を測定

**Precision@K**
- 上位K件中の関連文書の精度
- 値域：0-1（高いほど良い）
- 検索精度を測定

**nDCG@K (Normalized Discounted Cumulative Gain)**
- 順位を考慮した検索品質指標
- 値域：0-1（高いほど良い）
- 上位結果ほど重要視

**Hit Rate@K**
- 上位K件に関連文書が含まれるクエリの割合
- 値域：0-1（高いほど良い）
- 検索成功率を測定

## 技術仕様

### 非同期処理
- Streamlit環境での非同期評価実行
- 別スレッドでのバックグラウンド処理
- リアルタイム進捗表示

### エラーハンドリング
- 高度評価機能のフォールバック
- 簡易評価モードでの継続実行
- ユーザーフレンドリーなエラーメッセージ

### データ検証
- CSV形式の自動検証
- 不正データの検出と警告
- データ完整性チェック

## 使用例

### 基本的な評価フロー

1. **🎯 Evaluation** タブを開く
2. **📊 基本評価** を選択
3. **CSVアップロード** を選択し、評価データをアップロード
4. 評価設定を確認・調整
5. **🚀 評価を実行** をクリック
6. 結果を確認し、必要に応じてCSVダウンロード

### 手動データ入力での評価

1. **手動入力** を選択
2. テストケースを追加・編集
3. 質問と期待される引用元を入力
4. 評価設定を調整
5. 評価を実行

### 複数手法での比較評価

1. **📈 詳細分析** を選択
2. 比較したい類似度手法を複数選択
3. **グラフ可視化** をチェック
4. **📊 包括的評価を実行** をクリック
5. レーダーチャートと棒グラフで結果比較

### 単一質問でのテスト

1. **📋 カスタム評価** を選択
2. テスト質問を入力
3. 期待される回答を入力（任意）
4. 検索オプションを設定
5. **🔍 単一質問をテスト** をクリック
6. 回答と取得文書を詳細確認

## パフォーマンス最適化

### 評価速度向上
- `text_overlap`手法の使用（高速）
- 小さなK値での評価
- 評価データサイズの調整

### メモリ使用量削減
- バッチサイズの調整
- 大きなデータセットの分割処理
- 不要なデータの早期解放

## トラブルシューティング

### よくある問題

**評価が開始されない**
- Azure OpenAI APIキーの設定を確認
- データベース接続を確認
- evaluation_docsフォルダ内の文書が取り込まれているか確認

**評価結果が0.0000になる**
- 信頼性閾値が高すぎる可能性
- 評価データと取り込み文書の不一致
- 類似度計算手法を変更して再試行

**CSVアップロードに失敗する**
- ファイル形式の確認（UTF-8エンコーディング推奨）
- 必要なカラム名の確認
- データの文字化け確認

**メモリ不足エラー**
- 評価データサイズを削減
- K値を小さく設定
- ブラウザとStreamlitアプリの再起動

### デバッグ情報

評価実行時の詳細情報：
- 処理中の質問数と進捗
- 各質問の取得文書数
- 類似度スコアの分布
- API使用量とコスト

## 今後の改善予定

### 機能拡張
- より多様な評価指標の追加
- A/Bテスト機能
- 履歴管理とトレンド分析
- 自動レポート生成

### UI改善
- レスポンシブデザインの最適化
- より詳細な可視化オプション
- ドラッグ&ドロップでのファイルアップロード
- リアルタイム評価結果更新

### 性能改善
- 並列処理による高速化
- キャッシュ機能の実装
- ストリーミング評価結果表示
- 大規模データセット対応

## 関連ドキュメント

- [RAG評価用CSVデータ作成ガイド](./evaluation_csv_format.md) - CSV作成の詳細手順
- [RAGシステム評価ガイド](../EVALUATION_README.md) - 評価システムの全般的な説明

## まとめ

RAG評価UIは、コマンドライン評価の全機能をWebインターフェースで提供し、より直感的で使いやすい評価環境を実現しています。グラフィカルな結果表示、リアルタイム処理、柔軟な設定オプション、そして詳細なCSVデータ作成ガイドにより、RAGシステムの性能を効率的に分析・改善できます。